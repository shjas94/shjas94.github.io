---
title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
excerpt:

categories:
  - Paper
tags:
  - Computer Vision
  - Generative Model
  - GAN
use_math: true
toc: true
toc_sticky: true
last_modified_at: 2022-01-10T14:50:00-05:00
---

**Paper Link** : [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/pdf/1703.10593v7.pdf)

# Summary

* 생성된 이미지를 원본으로 되돌리는 Inverse Mapping $F : Y \rightarrow X$를 활용한 unpaired Image-to-Image Translation을 제안
* 위의 Inverse Mapping을 기존의 Loss term에 결합한 Cycle Consistency Loss를 제안

# Method

![image](https://user-images.githubusercontent.com/25663769/148680435-08d80ce8-2592-44f2-9bd4-c43ec6fd2348.png)


CycleGAN은 총 네 개의 네트워크로 구성되어 있다. Generator의 경우 Source Domain의 이미지를 Target Domain으로 매핑하는 $G : X \rightarrow Y$, Target Domain에서 다시 본래의 Source Domain으로 매핑하는 $F : Y \rightarrow X$ 로 구성되어 있다.

Discriminator 역시 이미지 $x$와 translated image $F(y)$를 구분하는 $D_X$, 그리고 $y$와 $G(x)$를 구분하는 $D_Y$로 구성되어 있다.

## Adversarial Loss

함수의 형태는 기존의 GAN Loss와 같다. 단, 차이점이 있다면 Loss term이 다음의 두 가지 항으로 구성된다는 것이다.

$$ \displaystyle{\mathcal{L}_{GAN}(G, D_Y, X, Y) = \mathbb{E}_{y\sim p_{data}(y)}[\log{D_Y(y)}] + \mathbb{E}_{x\sim p_{data}(x)}[\log{(1-D_Y(G(x)))}]} $$

$$ \displaystyle{\mathcal{L}_{GAN}(F, D_X, Y, X) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D_X(x)}] + \mathbb{E}_{y\sim p_{data}(y)}[\log{(1-D_X(F(y)))}]} $$

## Cycle Consistency Loss

기존의 Adversarial Loss 만으로는 output을 통제할 수 없다는 문제점이 있다. 구체적으로 말하자면, **input인 $x_i$를 desired output인 $y_i$로 mapping하는 것을 보장해주지 못한다**. 따라서, 이러한 문제점을 해결하기 위해 **Cycle Consistency Loss를 활용하여 mapping function $G$에 제약을 가한다**.

Cycle Consistent란, domain $X$의 이미지 $x$에 대하여 translate된 이미지 $G(x)$를 다시 원본 이미지 $x$로 **되돌릴 수 있어야 한다**는 것이다. 이를 수식으로 나타내면 다음과 같다. 

$$\displaystyle{x \rightarrow G(x) \rightarrow F(G(x)) \approx x}$$

$$\displaystyle{y \rightarrow F(y) \rightarrow G(F(y)) \approx y}$$

위의 수식은 translate된 이미지 $G(x)$를 다시 inverse mapping $F : Y \rightarrow X$통하여 원본 이미지에 근사시키는 역할을 하는 Loss function이라 볼 수 있고 이를 Forward Cycle Consistency라 부른다. 또한, 아래의 수식은 그 반대의 기능을 하는 것이라 볼 수 있으며 Backward Cycle Consistency라 부른다. 그리고 이러한 개념을 토대로 다음과 같이 Cycle Consistency Loss를 정의한다.

$$\displaystyle{\mathcal{L_{cyc}}(G, F) = \mathbb{E}_{x\sim p_{data}(x)}[\lVert F(G(x)) - x\rVert_1] + \mathbb{E}_{y\sim p_{data}(y)}[\lVert G(F(y)) - y\rVert_1]}$$

## Full Objective

위의 Adversarial Loss와 Cycle Consistency Loss를 결합한 Full Objective는 다음과 같다.

$$\displaystyle{\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}_{GAN}(G, D_Y, X, Y) +\mathcal{L}_{GAN}(F, D_X, Y, X) + \lambda\mathcal{L}_{cyc}(G, F)}$$

## Architecture

Generator로는 [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/pdf/1603.08155v1.pdf)에서 제시하는 아키텍처를 사용한다. 또한 Discriminator로는 [Pix2Pix](https://shjas94.github.io/paper/Pix2Pix/)에서 제시한 70x70 PatchGAN을 사용한다.

## Training Details

모델을 Training하는 과정에서 크게 두 가지 테크닉을 적용한다. 우선, Adversarial Loss 부분을 위와 같은 Cross-Entropy가 아닌 Least Square Loss로 변경하여 보다 더 안정적으로 training을 진행할 수 있도록 하였다. 변경한 Loss term은 다음과 같다.

$$ \displaystyle{\mathcal{L}_{LSGAN}(G, D_Y, X, Y) = \mathbb{E}_{y\sim p_{data}(y)}[(D_Y(y)-1)^2] + \mathbb{E}_{x\sim p_{data}(x)}[D_Y(G(x))^2]} $$

$$ \displaystyle{\mathcal{L}_{LSGAN}(F, D_X, Y, X) = \mathbb{E}_{x\sim p_{data}(x)}[(D_X(x)-1)^2] + \mathbb{E}_{y\sim p_{data}(y)}[D_X(F(y))^2]} $$

또한, 모델의 안정성을 높이기 위해서 [SimGAN](https://shjas94.github.io/paper/SimGAN/)에서 제시한 생성된 이미지의 history를 사용하여 Discriminator를 training하는 방법을 활용하였다.

# Experiment

## Analysis of Loss Term

![image](https://user-images.githubusercontent.com/25663769/148736323-ae8912f0-2dc5-4b29-98de-57757d5a8ca8.png)

![image](https://user-images.githubusercontent.com/25663769/148737308-6742b6f5-8abc-4fdb-943e-c7acea5072d1.png)

위의 Figure 7과 Table 5는 Loss들에 대한 Ablation Study를 수행한 결과이다. GAN Loss만을 사용했을때와 GAN Loss + Forward Loss의 경우는 얼핏 보면 괜찮은 결과를 출력하는 듯 보이고 아래의 Table 4의 결과를 보더라도 GAN Loss + Forward Loss 조합이 labels -> photos task에서 가장 높은 성능을 보이고 있지만, photos -> labels 결과물에서 Mode Collapse가 발생한 것을 볼 수 있다. 

따라서, 이러한 점을 고려하면 가장 좋은 결과물을 출력한 쪽은 GAN Loss와 Cycle Consistency Loss를 모두 활용한 CycleGAN이라는 결론을 내릴 수 있다. 

## Identity Loss

![image](https://user-images.githubusercontent.com/25663769/148751304-ddd0869e-b847-4d4b-820a-12d227bd095a.png)

또한, 저자들은 painting -> photo task에서 input의 color composition을 보존하기 위해 Identity Loss라는 것을 제안한다.

$$\displaystyle{\mathcal{L}_{identity}(G, F) = \mathbb{E}_{y\sim p_{data}(y)}[\lVert G(y) - y\rVert_1] + \mathbb{E}_{x\sim p_{data}(x)}[\lVert F(X)-X\rVert_1]}$$

Identity Loss란, mapping $G : X \rightarrow Y$와 $F : Y \rightarrow X$가 domain에서 크게 벗어난 출력을 생성하지 못하도록 억제하는 기능을 한다는 것을 위의 수식을 통해서 파악할 수 있다.

실제로도 위의 Figure 9을 보면, CycleGAN만으로 painting -> photo를 진행하였을 때는 input의 color compositon을 거의 보존하지 못하지만, Identity Loss term을 결합하였을 경우 input의 color composition을 거의 완벽하게 보존한 것을 확인할 수 있다.
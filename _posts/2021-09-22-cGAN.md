---
title: Conditional Generative Adversarial Nets
excerpt:

categories:
  - Paper
tags:
  - Computer Vision
  - Generative Model
  - GAN
use_math: true
toc: true
toc_sticky: true
last_modified_at: 2021-09-22T14:50:00-05:00
---

**Paper Link** : [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784v1.pdf)

# Abstract

본 논문에서는 GAN의 conditional한 버전이라고 볼 수 있는 Conditional Generative Adversarial Net에 대하여 설명한다. cGAN은 단순히 데이터 $y$를 입력에 추가함으로써, Discriminator와 Generator에 condition을 더해줄 수 있다.
<br/>
<br/>

# Introduction

GAN은 기존의 Generative Model들에 비해 연산 과정이 단순하고 확장 가능성이 높다는 장점이 있다. 하지만 뚜렷한 조건이 주어지지 않는 경우, 생성되는 데이터의 mode를 통제할 수 없다는 문제점이 있다.

하지만, 모델을 어떤 추가적인 정보에 대하여 conditioning 함으로써 data generation processs를 통제할 수 있다. 이러한 conditioning은 class label, inpainting에 활용되는 일부 데이터, 혹은 다른 modality의 데이터를 통하여 수행할 수 있다.
<br/>
<br/>

# Conditional Adversarial Nets

## Generative Adversarial Nets

- GAN에 대한 리뷰는 자세한 내용은 [GAN](https://shjas94.github.io/paper/GAN/) 참조

기존의 GAN은 두 가지 모델로 이루어져 있다. 데이터의 분포를 포착하는 Generative Model $G$와 sample이 $G$가 아닌 실제 훈련 데이터에서 나올 확률을 계산하는 Discriminative Model $D$로 이루어져 있다. 이 때, $G$와 $D$는 모두 MLP와 같은 Non-Linear Mapping Function이다.

데이터 $x$에 대한 Generator의 분포 $p_g$를 학습하기 위해서, Generator는 prior noise distribution $p_z(z)$로부터 데이터 공간 $G(z;\theta_g)$로의 mapping function을 만들어낸다. 그리고 Discriminator $D(x;\theta_d)$는 $x$가 $p_g$가 아닌 training data로부터 왔을 확률에 해당하는 스칼라 값을 output으로 내보낸다.

$G$와 $D$의 학습은 동시에 이루어지며 $G$의 경우는 $\log{(1-D(G(z)))}$를 최소화 하는 방향으로 파라미터를 최적화하며 $D$는 $\log{D(x)}$를 최소화하는 방향으로 진행된다. 이는 곧 어떤 value function $V(G, D)$에 대하여 two player minimax 게임을 수행하는 것과 같으며 이를 수식으로 표현하면 아래와 같다.

$$\displaystyle \min_G \max_D V(D, G) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D(x)}] + \mathbb{E}_{z\sim p_z(z)}[\log{(1-D(G(z)))}]$$
<br/>
<br/>

## Conditional Adversarial Nets

![image](https://user-images.githubusercontent.com/25663769/134348133-e611446d-e843-4f84-b602-87f433458893.png)

~~이제 논문의 메인디쉬인 cGAN에 대한 내용이다~~

cGAN은 기존의 GAN을 간단히 수정함으로써 구현할 수 있다. Discriminator와 Generator를 어떤 추가적인 정보 $y$를 투입함으로써 조건화 시키면 된다.
이 때, $y$는 보조적인 정보로써, 클래스 라벨이든 다른 modality에서 온 정보이든 상관 없다. 이제 이러한 추가 정보 $y$를 Discriminator와 Generator에 추가적인 정보로 넣어준다.

Generator에는 사전 노이즈 $p_z(z)$와 $y$가 joint hidden representation으로 결합되어 학습이 진행된다. Discriminator에서 역시 마찬가지로 기존의 input $x$에 $y$를 더한 정보가 input으로 주어진다.

위의 바뀐 정의들로 인하여 cGAN의 목적함수는 다음과 같이 변경된다.

$$\displaystyle \min_G \max_D V(D, G) = \mathbb{E}_{x\sim p_{data}(x)}[\log{D(x|y)}]+\mathbb{E}_{z\sim p_z(z)}[\log{(1-D(G(z|y)))}]$$
<br/>
<br/>

# Experimantal Result

![image](https://user-images.githubusercontent.com/25663769/134356626-3a40ec7d-abfb-4dc4-872f-bba738b1d0f6.png)

실험은 Unimodal, Multimodal 두 종류의 task에 대해서 진행하였다.
Unimodal의 경우는 Mnist 데이터를 생성하는 방식으로 실험을 진행하였다.
구체적으로 말하자면, one hot encoding된 라벨 $y$를 condition으로 추가해 주는 것이다.

결과를 보면 각 이미지가 conditon $y$에 잘 맞춰서 생성된 것을 볼 수 있다.
<br/>
<br/>

![image](https://user-images.githubusercontent.com/25663769/134356733-49c33c93-8fdf-4468-8c30-cd785ad1d3ba.png)

Multimodal 실험은 Image tagging task를 통하여 진행하였다. 이번에는 이미지를 condition으로 주고 tag를 생성하는 방식으로 실험을 진행하였고 위의 결과를 보면, 군데군데 잘못된 결과들이 보이긴 하지만 나름 condition image들에 대해 잘 맞춰진 것을 확인할 수 있다.

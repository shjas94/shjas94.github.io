---
title: Learning from Simulated and Unsupervised Images through Adversarial Training
excerpt:

categories:
  - Paper
tags:
  - Computer Vision
  - Generative Model
  - GAN
use_math: true
toc: true
toc_sticky: true
last_modified_at: 2021-10-18T14:50:00-05:00
---

**Paper Link** : [Learning from Simulated and Unsupervised Images through Adversarial Training](https://arxiv.org/pdf/1612.07828v2.pdf)

# Abstract / Introduction

본 논문에서 제시하는 내용은 다음과 같다. 
* Synthetic Image로부터 학습하는 Simulated + Unsupervised (S+U)라는 방법을 제시한다.
* Annotation 정보를 보존하고 artifact가 생기는 현상을 해결하며 보다 더 안정적으로 모델을 train하기 위한 방법들을 제시한다.
<br>
<br/>

![image](https://user-images.githubusercontent.com/25663769/137478156-41e2a761-ee24-476c-8047-3e2a55962c3d.png)

조금 더 디테일하게 살펴보자면, S+U learning이란 Synthetic Image를 Unlabeled Real Data를 이용하여 Simulator로부터 나온 Synthetic Image를 보다 더 Realistic하게 개선하는 것이다. 또한 이렇게 개선된 이미지는 이전의 annotation 정보들이 그대로 보존된다. 그리고 저자들은 Simulator로부터 나온 Synthetic Image를 Neural Network를 통해 refine하는 **'Refiner Network'** 로 명명하며, 이를 포함한 일련의 네트워크를 **SimGAN**이라 부른다.

Syntheric Image를 Realistic하게 정제하기 위하여 저자들은 GAN과 마찬가지로 Adversarial Loss를 사용한다. 즉, 해당 Loss를 통하여, Discriminator가 실제 이미지와 Refined된 이미지를 구분하지 못하도록 학습이 진행된다. 또한, Refine하는 과정에서 Synthetic Image의 annotation 정보를 보존하기 위하여 위의 Adversarial Loss에 Self-regularization term을 더한다. 이를 통하여, Synthetic Image가 지나치게 변형되는 것을 방지한다.

기존의 GAN은 상반되는 objective를 가진 두 개의 Neural Network를 학습하게 되는데, 그 과정이 불안정할 뿐더러, artifact를 생성한다는 문제점이 있었다. 저자들은 이러한 문제들을 해결하기 위해 두 가지 방안을 제시한다. 

Artifact의 생성을 방지하기 위하여, Discriminator의 Receptive Field를 제한한다. 즉, 이미지 전체가 아닌, 패치 단위로 쪼개진 이미지를 학습하게 되며 그 결과, 각 이미지마다 여러개의 Local Adversarial Loss가 생성된다. 

다음으로 학습의 안정성을 높이기 위하여 생성된 Refined Image를 학습에 한 번 사용하고 버리는 것이 아닌, 일종의 Buffer를 사용해 여러번 재활용하는 방안을 제시한다.

<br>
<br/>
 
# S+U Learning wigh SimGAN

S+U Learning의 목적은 Synthetic Image $\mathbf{x}$를 refine하는 Refiner $R_\theta(\mathbf{x})$에 라벨링되지 않은 이미지의 집합 
$$\mathbf{y}_i \in \mathcal{Y}$$
를 이용하는 것이다. refined된 이미지를 $$\tilde{\mathbf{x}}$$
로 정의하면, $\tilde{\mathbf{x}}:=R_\theta(\mathbf{x})$라는 식이 성립하게 된다. 이러한 사실을 기반으로, 저자들은 두 개의 결합된 loss term을 최소화 하는 방항으로 학습을 진행하여 Refiner의 파라미터 $\theta$를 학습하는 방법을 제시한다.

$$\displaystyle{\mathcal{L}_{\mathit{R}}(\theta) = \sum_{i} \mathcal{l}_{real}(\theta;\mathbf{x}_i, \mathcal{Y}) + \lambda\mathcal{l}_{reg}(\theta;\mathbf{x}_i)}$$ 

위의 수식에서 $$\mathbf{x}_i$$
는 $$i$$ 번째 synthetic training image를 의미한다. loss 함수에서 첫 번째 term인 $$l_{real}$$ 
는 synthetic image에 realism을 더해주는 부분이라 보면 된다. 그리고 두 번째 term인 $l_{reg}$ 는 synthetic 이미지의 annotation 정보를 보존해주는 term이라 볼 수 있다. 

## Adversarial Loss with Self-Regularization

이미지에 realism을 더하기 위해서는, Synthetic Image와 Real Image의 분포를 유사하게 만들어야 한다. 이상적인 Refiner가 생성한 이미지의 경우 실제 이미지와 구분하는 것이 거의 불가능할 것이다.

이러한 점에서 착안하여 Adversarial Discriminator Network $D_\phi$를 정의한다. Refiner Network $R$을 training하는 Adversarial Loss는 Discriminator Network $D$가 Refined Image와 Real Image를 구분할 수 없도록 만드는 역할을 한다.

전체 네트워크는 GAN의 접근방식을 따라간다. Two Player MiniMax Game을 수행하며, Refiner Network $R_\theta$와 Discriminator Network $D_\phi$를 교대로 학습하게 된다.

Discriminator Network는 아래의 loss함수를 최소화함으로써 파라미터 $\phi$를 최적화하며 Binary Cross-Entropy함수와 거의 동일한 역할을 한다고 볼 수 있다.

$$ \displaystyle{\mathcal{L}_D(\phi) = -\sum_{i}\log(D_\phi(\tilde{\mathbf{x}_{i}})) - \sum_{j}\log(1-D_\phi(\mathbf{y}_j))} $$


첫 번째 수식의 Realism Loss Function $\mathcal{l}_{real}$은 다음과 같다.

$$ \mathcal{l}_{real}(\theta; \mathbf{x}_{i},\mathcal{Y}) = -\log(1-D_\phi(R_\theta(\mathbf{x}_i))) $$

Refiner는 이러한 loss 함수를 최소화함으로써 Discriminator가 Refined Image를 synthetic으로 분류하지 못하도록 이미지를 Realistic하게 만든다.
하지만, Refiner는 이미지를 실제처럼 정제하는 것 외에도 Annotation 정보를 보존해야 한다. 이러한 제약 조건은 Refined Image를 다른 ML 모델에 활용하기 위해 필수적이라 할 수 있다. 

이를 위하여 저자들은 Self-Regularization Loss를 제시한다. Self-Regularization Loss는 feture transform을 수행한 Synthetic Image와 Refined Image 사이의 per-pixel difference를 최소화한다.
이를 수식으로 나타내면 다음과 같다.
$$\mathcal{l}_{reg} = \Vert \psi(\tilde{\mathbf{x}}) - \mathbf{x} \Vert_1$$
$\psi$는 image space에서 feature space로의 mapping이며 $\Vert\ldotp\Vert_1$는 $\text{L}1$ norm을 의미한다.
Feature Transformation으로는 identity mapping, image derivatives, color channel의 평균, 혹은 CNN을 통해 학습된 transformation을 사용할 수 있다.(논문에서 사용하는 것은 identity mapping임)

이제 맨 위에서 설명한 수식을 다음과 같이 풀어쓸 수 있다.
$$ \displaystyle{\mathcal{L}_R(\theta) = -\sum_i \log(1-D_\phi(R_\theta(\mathbf{x}_i))) + \lambda\Vert \psi(R_\theta(\mathbf{x}_i)) - \psi(\mathbf{x}_i) \Vert_1} $$

Refiner와 Discriminator는 $\mathcal{L}_R(\theta)$와 $\mathcal{L}_D(\phi)$를 교차로 최소화하는 방식으로 학습된다. 전체적인 과정은 다음과 같다.

![image](https://user-images.githubusercontent.com/25663769/137636780-439051c6-2694-48c8-8490-1369f073fb12.png)

## Local Adversarial Loss

Refiner Network의 또다른 제약조건은 artifact 없이 Real Image의 특성을 학습해야 한다는 것이다. 뛰어난 성능의 Discriminator Network를 학습하게 되면, Refiner Network는 Discriminator Network를 속이기 위해서 특정한 이미지의 feature들을 과하게 강조하게 되고 이로 인하여 artifact가 생겨나게 된다. 

한가지 중요한 포인트는 어떤 local patch가 Refined Image에서 sampling되더라도 Real Image patch와 비슷한 통계치를 가지고 있어야 한다는 것이다. 저자들은 이러한 점에서 착안하여, Global Image를 판별하는 것이 아닌, local patch들을 판별하도록 Discriminator를 구성하였다. 이는, Discriminator Network의 receptive field와 capacity를 제한하는 것 뿐 만이 아니라, Discriminator Network가 가능한 많은 sample들에 대해서 학습할 수 있도록 한다. 

![image](https://user-images.githubusercontent.com/25663769/137638329-a9efd4e9-07f0-418a-b471-1327d1f63519.png)

## Updating the Discriminator using a History of Refined Images

기존의 Adversarial Training의 문제점은 Discriminator가 가장 최근의 Refined Image만을 보게 된다는 것이다. 이러한 문제로 인하여 Adversarial Training 과정 자체가 발산해버릴수도 있고 Refiner Network가 artifact를 다시 생성해버리는 문제가 발생할 수 있다. 

네트워크의 훈련 과정에서 생성되는 모든 Refined Image는 모두 **fake image**이고 Discriminator는 당연히 이들을 모두 fake로 분류할 수 있어야 한다. 저자들은 이러한 점에서 착안하여 Adversarial Training의 안정성을 높이기 위한 방법으로 **Refined Image의 history를 이용한 방법**을 제시한다.

![image](https://user-images.githubusercontent.com/25663769/137638669-cfda55b0-1a02-460a-aa38-cb861dba270a.png)

Refined Image 들을 저장하기 위한 일종의 버퍼를 구성하고 $B$를 버퍼의 크기, $b$를 mini-batch의 크기로 정의한다. 각 iteration마다 Discriminator Network를 학습할 때, $b/2$ 만큼의 이미지를 현재 Refiner Network로부터 Sampling하고, 나머지 절반을 버퍼에서 샘플링하며 이들로부터 Discriminator Loss를 계산한다.

그리고 각 iteration이 끝날 때마다 버퍼에서 $b/2$만큼의 샘플을 무작위로 제거하고 새로운 Refined Image로 채워나간다.